{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO73PxdnjOV7Hxjts+ynEKQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NimrodDev/TensorFlow/blob/main/RNN_Play_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAMpgNwz7BP2",
        "outputId": "05519cca-dc32-4e6e-c998-b4bb72a0fdd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is our dataset\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "I3pZT9A58Xlh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we look at the content of the file\n",
        "# Read, then decode for py2 compat\n",
        "text = open(path_to_file, 'rb').read().decode(encoding= 'utf-8')\n",
        "# Length of text is the number of characters in it\n",
        "print('Length of the text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIc7iJDILNOW",
        "outputId": "33abdd40-5810-4198-abaa-5eae547e5673"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5raWzx9M2AW",
        "outputId": "e17c067a-24de-468d-a6ed-e3e676807d3c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "# Here we create a map from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "id": "AAPzaHBaOlVC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets look at how part of our text is encoded\n",
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDATQPYXT-Qn",
        "outputId": "b15f574c-5e05-477a-cc91-7f9129a561e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IIn0RhIUcnf",
        "outputId": "1bf74f1a-8365-4777-f43e-adbc452432a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we create training\n",
        "seq_length = 100 # Length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "metadata": {
        "id": "1basypD7VK_I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder= True)"
      ],
      "metadata": {
        "id": "i21qvD5eXnNH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk): # For the example: Hello\n",
        "  input_text =chunk[:-1] # hell\n",
        "  target_text = chunk[1:] # ello\n",
        "  return input_text, target_text # hell, ello\n",
        "\n",
        "\n",
        "dataset = sequences.map(split_input_target) # We use map to apply the above function to every entry\n"
      ],
      "metadata": {
        "id": "0ujo6KmdYImD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aKwl2yTAS6o",
        "outputId": "db0133b2-cb34-4f43-8de1-18a808933f75"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab) # Vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "#Buffer size to shuffle the dataset\n",
        "'''TF data is designed to work with possibly infinte sequence\n",
        "So it doesn't attempt to shuffle the entire sequence in memory, instead.\n",
        "It maintains a buffer in which it shuffles elements.'''\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder= True)"
      ],
      "metadata": {
        "id": "p9XfOwqtBg6l"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we build the model\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                batch_input_shape=[batch_size, None]),\n",
        "      tf.keras.layers.LSTM(rnn_units,\n",
        "                           return_sequences= True,\n",
        "                           stateful= True,\n",
        "                           recurrent_initializer= 'glorot_uniform'),\n",
        "      tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JSP_UsCI4UG",
        "outputId": "d6e3f453-c261-4ce7-9b5f-deca8f39c244"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5330241 (20.33 MB)\n",
            "Trainable params: 5330241 (20.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape) # Prints out the output shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0BAH1wONvMN",
        "outputId": "4188cbcc-0db8-45d3-ffa6-bf8d21814756"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see that prediction is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jfsGDhlb769",
        "outputId": "9203b192-e081-4dd2-b32a-749ceb6ae1fe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-9.53555107e-04 -5.43494057e-03  1.66971143e-03 ...  3.53426463e-03\n",
            "    1.32870628e-03  1.11273467e-03]\n",
            "  [ 1.89250568e-03 -2.20255926e-03 -2.99942167e-03 ...  4.84376727e-03\n",
            "    1.16049789e-03 -1.87275698e-03]\n",
            "  [ 2.60339747e-03 -6.33168686e-03 -8.92124022e-04 ...  2.36065476e-03\n",
            "    3.17339203e-03 -8.32510088e-03]\n",
            "  ...\n",
            "  [-2.86091212e-03 -7.13585760e-05 -2.26686499e-03 ...  1.33764395e-03\n",
            "    3.73524521e-03  1.63650699e-03]\n",
            "  [-1.28315436e-03  5.95728587e-03  1.00507052e-03 ...  4.30892082e-03\n",
            "    5.91445388e-03  2.60841614e-03]\n",
            "  [-7.33029656e-03  2.13686936e-03  3.06039560e-03 ... -5.12046041e-04\n",
            "    3.15929530e-03  5.04571106e-03]]\n",
            "\n",
            " [[-4.67534550e-03  3.27738002e-04 -2.94020912e-03 ... -3.76581121e-03\n",
            "    1.92881876e-03  1.71950355e-03]\n",
            "  [ 1.05416728e-03  3.61941382e-03  1.62655488e-05 ... -7.37315277e-03\n",
            "    2.23714672e-03  1.88839587e-03]\n",
            "  [ 5.47171850e-03  6.63154759e-03  2.91790068e-03 ... -9.27686412e-03\n",
            "    3.55854374e-03  1.97287882e-03]\n",
            "  ...\n",
            "  [-2.16504745e-03  5.90843940e-03  8.08178913e-04 ... -4.79693478e-03\n",
            "    3.80527880e-03  1.56010350e-03]\n",
            "  [-3.89551185e-03 -1.92126969e-03  1.65807153e-03 ... -4.24614502e-03\n",
            "   -1.37828710e-03 -2.09378684e-03]\n",
            "  [-1.30347838e-03  1.01807178e-03 -2.63190852e-03 ... -3.57818860e-03\n",
            "   -1.16885640e-04 -3.50226881e-03]]\n",
            "\n",
            " [[-1.19314727e-03 -4.16107848e-03  4.17091930e-03 ...  7.60039967e-03\n",
            "    6.86645741e-04 -1.75948907e-03]\n",
            "  [-3.05713154e-03 -8.26972630e-03  1.91123178e-03 ...  6.88385125e-03\n",
            "   -4.29912843e-03 -1.92304444e-03]\n",
            "  [-1.43252627e-03 -6.00316282e-03 -5.91614656e-03 ...  3.74561083e-03\n",
            "   -3.87416966e-03 -3.06033692e-03]\n",
            "  ...\n",
            "  [ 3.45558953e-03  2.06029695e-03 -3.05176806e-03 ...  7.17187067e-03\n",
            "   -2.00294424e-04  3.86706903e-03]\n",
            "  [ 3.70457722e-03  6.41711522e-03 -5.40882908e-03 ...  2.86915479e-03\n",
            "   -1.68294972e-03  4.99502895e-03]\n",
            "  [ 4.84149437e-04  8.85670166e-03 -6.37091650e-03 ...  5.89883700e-03\n",
            "   -6.29928894e-04  6.00433256e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 6.63668849e-04  2.93857884e-04 -6.46598497e-03 ... -2.03031627e-03\n",
            "    2.77807470e-04 -2.30188132e-03]\n",
            "  [-7.76962610e-04 -4.86633182e-03 -5.34284580e-03 ... -3.86478321e-04\n",
            "   -2.96505704e-03 -3.22092557e-03]\n",
            "  [-5.70857571e-03 -5.02457749e-03 -9.29363130e-04 ... -3.13788839e-03\n",
            "   -3.24173388e-03  3.09312716e-04]\n",
            "  ...\n",
            "  [ 2.63771275e-04  6.28546020e-03  2.98096030e-03 ...  6.30437955e-03\n",
            "    3.76174925e-03  1.01009822e-02]\n",
            "  [ 5.48987882e-04  6.15030248e-03  2.48976564e-03 ... -3.38771241e-03\n",
            "    4.71803593e-03  5.07516460e-03]\n",
            "  [ 3.05359648e-03  6.59990916e-03 -4.54382971e-04 ...  5.76226553e-03\n",
            "    3.63616366e-03  2.14826409e-03]]\n",
            "\n",
            " [[ 8.11306760e-04  1.98623165e-05 -8.38368200e-04 ... -1.98456994e-03\n",
            "   -5.21423202e-03  5.12486091e-03]\n",
            "  [ 4.19414137e-05 -2.24041985e-03  2.25205044e-03 ... -6.58153370e-03\n",
            "   -4.05869633e-03  5.70493331e-03]\n",
            "  [-2.42581125e-04  1.30519707e-04  2.35739746e-03 ... -1.32517945e-02\n",
            "   -1.16264541e-03  1.36985909e-03]\n",
            "  ...\n",
            "  [ 3.98505596e-04  5.39357774e-03 -1.02453772e-02 ...  8.04962311e-03\n",
            "    5.49615500e-03 -1.26485992e-03]\n",
            "  [ 2.02035462e-03  9.63146705e-03 -5.54363243e-03 ...  8.07456300e-03\n",
            "    6.58960780e-03 -2.15483829e-04]\n",
            "  [ 4.04276140e-03  7.76784914e-03 -7.63670681e-03 ...  1.27620399e-02\n",
            "    3.94325517e-03 -2.86007347e-03]]\n",
            "\n",
            " [[-2.67749955e-03  5.14571415e-03 -1.24302856e-03 ...  3.28520173e-03\n",
            "    1.29728997e-03  1.57960108e-03]\n",
            "  [-3.90828296e-04  1.14559736e-02  2.07161880e-03 ...  5.91870863e-03\n",
            "    3.97006702e-03  2.10264209e-03]\n",
            "  [ 3.13406112e-04  9.27040819e-03  1.80143665e-03 ... -3.85247543e-03\n",
            "    4.48086346e-03 -1.73575873e-03]\n",
            "  ...\n",
            "  [ 1.35762210e-03  1.01016182e-02  3.86526110e-04 ... -4.14756406e-03\n",
            "    2.36234255e-03  5.96282538e-03]\n",
            "  [-4.77148918e-04  3.54094943e-03  1.30484917e-03 ...  1.12061272e-04\n",
            "    2.70811515e-03  6.07660972e-03]\n",
            "  [ 6.85190316e-05  4.19462146e-03 -3.43065709e-04 ... -1.08870864e-03\n",
            "   -3.64238815e-03  1.01159802e-02]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# Notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time ste"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USGsFq42cvt4",
        "outputId": "86b74218-c7ae-40df-99da-006b5f0deacb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-9.5355511e-04 -5.4349406e-03  1.6697114e-03 ...  3.5342646e-03\n",
            "   1.3287063e-03  1.1127347e-03]\n",
            " [ 1.8925057e-03 -2.2025593e-03 -2.9994217e-03 ...  4.8437673e-03\n",
            "   1.1604979e-03 -1.8727570e-03]\n",
            " [ 2.6033975e-03 -6.3316869e-03 -8.9212402e-04 ...  2.3606548e-03\n",
            "   3.1733920e-03 -8.3251009e-03]\n",
            " ...\n",
            " [-2.8609121e-03 -7.1358576e-05 -2.2668650e-03 ...  1.3376439e-03\n",
            "   3.7352452e-03  1.6365070e-03]\n",
            " [-1.2831544e-03  5.9572859e-03  1.0050705e-03 ...  4.3089208e-03\n",
            "   5.9144539e-03  2.6084161e-03]\n",
            " [-7.3302966e-03  2.1368694e-03  3.0603956e-03 ... -5.1204604e-04\n",
            "   3.1592953e-03  5.0457111e-03]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FInally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# Its 65 values representing the probability of each character accuring next"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U39EWTAdd-f2",
        "outputId": "218d7b12-27b7-4d2f-8fef-d2228738b667"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-9.5355511e-04 -5.4349406e-03  1.6697114e-03 -1.7866559e-03\n",
            "  2.4600828e-03 -1.6624543e-03  9.9329266e-04 -1.3200623e-03\n",
            "  6.3031825e-05  6.5862946e-03  6.7458041e-03  3.4296159e-03\n",
            "  1.0350549e-03 -2.9278686e-03 -2.0644106e-03  8.3867274e-04\n",
            "  2.5963085e-03 -7.9236284e-04 -5.0064488e-03  5.0838920e-04\n",
            " -1.5255702e-03 -2.0408265e-03  8.3077629e-04  7.4615318e-04\n",
            "  1.5583599e-03  2.5007981e-03  5.2456750e-04  3.9360048e-03\n",
            "  4.6492489e-03 -3.7979968e-03  5.6849309e-04 -3.8482677e-03\n",
            " -1.9216794e-03  5.0619314e-03  5.3372089e-04  8.0431076e-03\n",
            " -4.5262335e-04 -6.0866482e-04  7.5346325e-03  3.7554770e-03\n",
            "  1.5133934e-03  5.0661284e-03  2.6127354e-03  1.8597584e-03\n",
            "  1.5738355e-04 -9.2232286e-04  9.4832521e-04  4.4974163e-03\n",
            " -1.2398171e-03 -2.7429275e-04 -5.1549561e-03  9.4863790e-04\n",
            " -4.1169934e-03  4.7797733e-04  3.2755202e-03 -8.5362117e-04\n",
            "  4.2046560e-03  4.1138497e-04  5.6123943e-04  2.0713487e-03\n",
            " -2.7899221e-03  4.6606776e-03  3.5342646e-03  1.3287063e-03\n",
            "  1.1127347e-03], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# Now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars # This is what the model predicted for training sequence 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "i84QQx6ie01Z",
        "outputId": "30415bd4-2da9-4f95-8fbb-4074cb350b07"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lbIyZ:vbvoO,hNgxhfc3XevP XMh$iaNdU-pHwMVqfdIKha-EQH\\niEwEjWXXHDr!LZmpABRLcYqApmEaUOuzQIr WKiTz?cz$Hdu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "x0Yx7sNBvmhm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we compile our model\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "rw4qM3lxxCb7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback= tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only= True)"
      ],
      "metadata": {
        "id": "RWHVOXV2xxNN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally we start training our model\n",
        "history = model.fit(data, epochs=10, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSbO3yLr6xFl",
        "outputId": "805266ed-2a5c-4d0c-933e-f1a10068f3b0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 21s 84ms/step - loss: 2.5471\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 16s 82ms/step - loss: 1.8635\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 16s 82ms/step - loss: 1.6207\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 15s 80ms/step - loss: 1.4928\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 1.4134\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 16s 77ms/step - loss: 1.3568\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 15s 81ms/step - loss: 1.3129\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 16s 82ms/step - loss: 1.2733\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 15s 78ms/step - loss: 1.2371\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 15s 78ms/step - loss: 1.2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we load our model\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size = 1)"
      ],
      "metadata": {
        "id": "xAx7_MsHBu8G"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "-wL6vIzTRk5y"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can load any checkpoint we went want by specifying the exact file to load\n",
        "checkpoint_num = 10\n",
        "\n",
        "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "W8cdXSJ_PFWu",
        "outputId": "35c6d5d6-db20-45e9-dfe1-43c6e128fc05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'tensorflow.python.util._pywrap_checkpoint_reader.C' object has no attribute 'endswith'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-190e538d51d0>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./training_checkpoints/ckpt_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/saving_utils.py\u001b[0m in \u001b[0;36mis_hdf5_filepath\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     return (\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0;32mor\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mor\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.util._pywrap_checkpoint_reader.C' object has no attribute 'endswith'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  #Converting our start string to numbers(vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures result in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch_size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # Remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # Using a categorical distribution to predict the character returned by the model\n",
        "    predictions =predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    '''We pass the predicted character as the next input to the model'''\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "Y_fGCDC9XPxV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input(\"Type a starting string:\")\n",
        "print(generate_text(model, inp))"
      ],
      "metadata": {
        "id": "AbcSZrSo0F_g",
        "outputId": "ecf63a21-8859-4fee-e5f9-feb6a5df0a70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string:romeo\n",
            "romeous great sovereign:\n",
            "High power will swear by rooting kings,\n",
            "It shuds i' the sun hath thy custombas's the bed the imprison!\n",
            "\n",
            "WARWICK:\n",
            "We are se; commends thee in the beggard\n",
            "Calling of CarubaMa mayarnight hide thee,\n",
            "And enter gait wither and their tribbed soul,\n",
            "And in the gross his power to?\n",
            "Was I think so, sir, and as his wrong\n",
            "As comes your ears and night\n",
            "That noble and will I kind their ey alives\n",
            "For heaven and such a slaughter\n",
            "Deserve all royalty; good careless dog!\n",
            "Come, sirs, let's take off in himself my rebellion;\n",
            "A meeting thing that he is s death, this is my loving bre.\n",
            "\n",
            "RIVERS:\n",
            "What save your brood, my lord.\n",
            "\n",
            "Second Saint well, now what is bold, that Julium\n",
            "As yours I'll sullen in this new head which be denied them.\n",
            "\n",
            "YORK:\n",
            "What should in jession will hear pardon?\n",
            "\n",
            "LADY CAPULET:\n",
            "Fe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2sS2fTHT2RKa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}